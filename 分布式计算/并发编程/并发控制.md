## Semaphore(信号量)

信号量又称为信号灯，它是用来协调不同进程间的数据对象的，而最主要的应用是共享内存方式的进程间通信。本质上，信号量是一个计数器，它用来记录对某个资源(如共享内存)的存取状况。一般说来，为了获得共享资源，进程需要执行下列操作:
　　 (1) 测试控制该资源的信号量。
　　 (2) 若此信号量的值为正，则允许进行使用该资源。进程将信号量减 1。
　　 (3) 若此信号量为 0，则该资源目前不可用，进程进入睡眠状态，直至信号量值大于 0，进程被唤醒，转入步骤(1)。
　　 (4) 当进程不再使用一个信号量控制的资源时，信号量值加 1。如果此时有进程正在睡眠等待此信号量，则唤醒此进程。

信号量与普通整型变量的区别：
① 信号量(semaphore)是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问；
② 操作也被成为 PV 原语(P 来源于 Dutch proberen"测试"，V 来源于 Dutch verhogen"增加")，而普通整型变量则可以在任何语句块中被访问；
信号量与互斥锁之间的区别：

1. 互斥量用于线程的互斥，信号线用于线程的同步。

这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。

互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。

同步：是指在互斥的基础上(大多数情况)，通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源

Semaphore 是多线程编程时一个很重要的概念，概念本身并不复杂，但要做到正确使用却不容易。这里我们从面向对象的角度来理解这个概念。现实生活中的任何对象或实体，我们都可以用 class 来描述它。Semaphore 也不例外，如果用 class 定义应该是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_semaphore.png?raw=true)

Semaphore 对象里包含一个 count 值和一个队列对象，另外有两个对外 public 的方法，wait()和 signal()，需要特别注意的事，count 值代表的资源数量是不能为负的。为了理解这些属性和方法，我们可以类比一个现实生活中的例子。大家去餐厅吃饭，假设这个餐厅有 10 个座位，有 20 个吃货随机出发去这个餐厅吃饭，那么对应关系是这样的：

- count = 10，10 个座位。
- queue，餐厅位置有限，为了避免混乱，餐厅肯定会吃货们排队。
- wait()，吃货到了餐厅找服务员要位置点餐，这个行为就是 wait。
- signal()，吃货吃完了买单离开位置，这个行为就是 signal。

这其实是一个信号量应用的典型场景，这里关键在于正确理解 wait 和 signal 发生时都有哪些细节步骤。用代码来描述大概是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/semaphore_wait_signal.png?raw=true)

**wait**

具体到餐厅到例子，20 个人随机出发去餐厅吃饭，有 10 个人先到，然后挨个执行 wait。前 10 个人执行 wait 的时候是有位置的，所以 count>0，这 10 个人每人都消耗掉一个座位开始吃饭。到第 11 个人到了都时候，count ＝＝ 0，没有位置了，所以被 suspend，开始加入排队都队列等待。后续所有人都慢慢的到来，但和第 11 个人一样，都只能排队。

**signal**

过了一段时间之后，有个人吃好结账离开了餐厅。这时候如果没有人在排队，位置数量 count ＋＋，没有其它事情发生。但如果有人在排队，比如上面的情况，有 10 个人在等待位置，餐厅会把排在第一个的人安排到刚才空出来的位置，count 值没有变化，但队列的人少了一个。

**特别注意**

对于 wait 和 signal 还有两点需要特别注意。也是平时我们使用 semaphore 时比较容易产生 bug 的地方。

- wait 和 signal 都是原子操作。可以简单理解为上面代码里 wait(),signal()两个函数都是加锁的。这个特性其实让 semaphore 的行为变得更简单清晰。大家想象，如果到餐厅的 10 个人是同时到达的，但不是依次询问餐厅是否有位置，而是 10 张嘴同时说话，同时找餐厅要位置，显然情况会变得复杂不好处理。
- wait 或者 signal 调用的顺序是不确定的。上面的例子中每个人都是随机时间出发，到达餐厅的顺序也是随机的，并不一定先出发的就先到。同理每个人吃饭的时间长短也不一定，有人快有人慢，所以吃好离开餐厅的时间点也是随机的。这里每个人都代表一个线程，因为操作系统线程调度策略导致到底哪个线程先执行也是不确定的。

## Mutex(互斥量)

理解了 Semaphore，再看 Mutex 就很简单了。可以把 Mutex 理解成 count ＝＝ 1 的 Semaphore。在使用 Mutex 的场景下，永远都只允许有一个线程在占有资源，其它的线程都必须等待。建议大家按照 count ＝ 1 把上面 Semaphore 的例子再在脑子里过一遍，加深理解。

## Lock(锁)

上面说的 Semaphore 和 Mutex 都是操作系统层面的基础概念。但具体到某个平台的时候，平台会对这两个概念再做一次封装以方便使用。比如在 iOS 上就有 NSLock 这个类，提供 lock(),unlock()两个功能。但其实 Lock 在概念上和 Mutex 是一致的。当然平台既然做了封装就会提供额外的功能或者做一些额外的处理。这也是为什么在 iOS 里，NSLock 的性能会比 pthread_mutex 会差一些。iOS 里各种锁性能对比可以参考[这篇文章](http://perpendiculo.us/2009/09/synchronized-nslock-pthread-osspinlock-showdown-done-right/)。

## Condition(条件变量)

另一个遇到机会相对较少的概念是 Condition，Condition 理解起来很容易和上面几个概念混淆，但是只要和 Semaphore 对比下不同之处理解就很简单了。Condition 甚至可以理解成一种“特殊”的 Semaphore。特殊之处就在于它没有 count(资源数)。它也有 wait 和 signal 两种行为。用代码简单表示是这样的：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_condition.png?raw=true)

逻辑其实变更简单了，可以从事件的角度去看待 Condition。每次 condition 调用 wait 的时候，表示它想等待某个事件的发生(不管之前有没有发生过)，所以一定是加入到等待队列当中。调用 signal 的时候，表示这个事件发生了，如果有线程在队列里等待，则取出其中一个来执行，后面的继续等待后续事件。如果队列是空的，这个事件就丢了，当什么也没有发生过。所以这里的关键在于对 count(资源)和事件的理解。

## 可重入锁

锁作为并发共享数据，保证一致性的工具，在 JAVA 平台有多种实现(如 synchronized 和 ReentrantLock 等等 ) 。这些已经写好提供的锁为我们开发提供了便利，但是锁的具体性质以及类型却很少被提及。本系列文章将分析 JAVA 下常见的锁名称以及特性，为大家答疑解 惑。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。
在 JAVA 环境下 ReentrantLock 和 synchronized 都是 可重入锁。

```
public class Test implements Runnable{
 public synchronized void get(){
  System.out.println(Thread.currentThread().getId());
 //在子方法里又进入了锁
  set();
 }
 public synchronized void set(){
  System.out.println(Thread.currentThread().getId());
 }
 @Override
 public void run() {
  get();
 }
 public static void main(String[] args) {
  Test ss=new Test();
  new Thread(ss).start();
  new Thread(ss).start();
  new Thread(ss).start();
 }
}


```

两个例子最后的结果都是正确的，即 同一个线程 id 被连续输出两次。
结果如下：

```
Threadid: 8
Threadid: 8
Threadid: 10
Threadid: 10
Threadid: 9
Threadid: 9
```

可重入锁最大的作用是避免死锁。
我们以自旋锁作为例子。

```
public class SpinLock {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 public void lock(){
  Thread current = Thread.currentThread();
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  owner.compareAndSet(current, null);
 }
}

```

对于自旋锁来说：
1、若有同一线程两调用 lock() ，会导致第二次调用 lock 位置进行自旋，产生了死锁
说明这个锁并不是可重入的。(在 lock 函数内，应验证线程是否为已经获得锁的线程)
2、若 1 问题已经解决，当 unlock()第一次调用时，就已经将锁释放了。实际上不应释放锁。
(采用计数次进行统计)

```
public class SpinLock1 {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 private int count =0;
 public void lock(){
  Thread current = Thread.currentThread();
  if(current==owner.get()) {
   count++;
   return ;
  }
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  if(current==owner.get()){
   if(count!=0){
    count--;
   }else{
    owner.compareAndSet(current, null);
   }
  }
 }
}
```

# Lock Memory Model(锁的内存模型)

在多处理器下，多个处理器共享主存。为了效率并不要求处理器将更新立即同步到主存上。处理器拥有自己的缓存以保存这些更新，并且定期与主存同步。这种需要 定期同步的方式是为了保证缓存一致性(Cache Coherence)。在缓存一致性许允许的范围内，多个处理器可以拥有同一个共享数据的不同状态。内存模型提供了一种保证：规定共享数据在不同线程间的 状态总是一致的。它的复杂性在于要协调处理器和编译器在与多线程程序执行时的性能与数据同步状态之间的平衡。处理器和编译器的工作是通过优化指令执行顺序 添加缓存来加快指令执行速度。内存模型采用一组屏障指令来保证存储的一致性，当然是在尽可能少的牺牲性能的前提下。具体的编译器和处理器加快指令执行的方 法是代码重排、指令重排以及缓存。内存模型利用处理器提供的一组指令来保护数据一致性，这种方式称为内存屏障(Memory Barriers)。

## 代码重排

代码重排是指编译器对用户代码进行优化以提高代码的执行效率，优化前提是不改变代码的结果，即优化前后代码执行结果必须相同。譬如：

```
int a = 1, b = 2, c = 3;
void test() {
    a = b + 1;
    b = c + 1;
    c = a + b;
}
```

在 gcc 下的汇编代码 test 函数体代码如下：
编译参数: -O0

```

movl b(%rip), %eax
addl $1, %eax
movl %eax, a(%rip)
movl c(%rip), %eax
addl $1, %eax
movl %eax, b(%rip)
movl a(%rip), %edx
movl b(%rip), %eax
addl %edx, %eax
movl %eax, c(%rip)
```

编译参数：-O3

```

movl b(%rip), %eax                  ;将b读入eax寄存器
leal 1(%rax), %edx                  ;将b+1写入edx寄存器
movl c(%rip), %eax                  ;将c读入eax
movl %edx, a(%rip)                  ;将edx写入a
addl $1, %eax                       ;将eax+1
movl %eax, b(%rip)                  ;将eax写入b
addl %edx, %eax                     ;将eax+edx
movl %eax, c(%rip)                  ;将eax写入c
```

我在-O3 的汇编下做了详细的注释，参照注释和原 C 代码理解这两段汇编代码应该不难。当然编译器优化并没有做多少工作，这是因为并未有多少无用代码。但是如果我们的 test 函数体内只写了 100 行的 a++; 那汇编指令使用-O1 就会优化这 100 行代码成一条：

```
addl $100, a(%rip)
```

然而，上面的代码更有意义来说明编译器优化并且其中将后面可能用到的汇编指令做了清楚的注释以说明其含意，便于下文对汇编代码的理解。如果觉得上述代码的指令重排难于理解或是不够充分，接下来看这段 C 代码和 gcc -O3 下的汇编代码：

```

int a = 1, b = 2;
void test1() {
    a = b + 1;
    b = 39;
}
------
movl b(%rip), %eax ;读取b给eax
movl $39, b(%rip) ;向b写入39
addl $1, %eax ;eax+1
movl %eax, a(%rip) ;将eax写回a
```

很显然 b 先被赋值，然后才是 a。也就是说在该函数中，代码的执行顺序发生了变化，但却不影响最终结果。编译器重排是根据指令之间是否有数据依赖关系来决定 的，虽然看似 C 代码间存在依赖，但是重排却是指令级别的。顺便分析下这里为什么会把 b 的赋值操作提前进行呢？寄存器读入 b 的值时对 b 进行缓存，再写入的时 候直接从寄存器缓存中取出赋值避免了再次从高速缓存甚至主存中取出 b 再赋值。可以动手写实验代码验证，很容易发现确实编译器会将相关变量的操作提取到一起 执行，这是因为处理器充分利用寄存器缓存来加速指令执行。

## 指令重排

大多数主流的处理器为了效率可以调整读写操作的顺序，但为什么这么做呢？处理器在执行指令期间，会把指令按照自适应处理的最优情形进行重新排序，使指令执 行时间变得更短(绝大多数情形下，前提是不改变程序的执行结果)。处理器的具体做法是优化其指令流水线(Instruction pipeline)以减少指令执行时间。
我们假设在一条简单的流水线中，完成一个指令可能需要 5 层。对于一些并不互相依赖的指令，要在最佳性能下运算，当第一个指令被运行时，这个流水线需要运行 紧接着的 4 条独立的指令。如果有指令依赖前面已经执行的指令，那处理器就会通过某种方式延缓该条指令执行直到依赖的指令执行完毕。使用多个层执行指令，处 理器可以显著提高性能从而减少指令运行所需要的周期。处理器使用这种优化 Pipeline 的方式一方面提高了指令执行效率，但另一方面却出现了另一个麻 烦。单处理器下执行指令调整顺序在多线程并发的时候出现了困难。我们假设有两个处理器，每一个处理器执行一条线程，对于涉及到同一段代码对非局部变量赋值 的顺序会因为的每一个处理器对各自指令顺序调整而变得混乱。
看下启动服务器的示例代码：
全局：bool enable = false;

```

void start_server() {
    open_server();
    enable = true;
}

void http_server() {
    if(enable) handle_request();
}
```

我们使用两条线程在多处理器下并发，一条线程 A 负责启动服务器 start_server(),另一条线程 B 负责处理 http 请求。由于上述所说的处理器会 将互不依赖的两条指令调换顺序(这里暂且忽略编译器优化)，我们有理由相信会出现这样的情形：A 线程：enable = true;已经执行，但 open_server();还未调用。此时另一条线程 B 执行 http_server();发现服务可用，直接使用未打开的服务器 来处理请求。
同样的我们如果在此忽略处理器重排，只使用编译器的代码重排也会导致上述问题。问题的复杂性在于编译器和处理器同时作用下，指令的执行顺序更加神秘莫测。 但更糟糕的是还有一种为了效率缓存指令执行结果使数据不能及时更新的因素影响数据同步，这个因素就是 CPU Cache。
