# 爬虫

看了回答区，基本的反爬虫策略都提到了，下面说几个作为补充。

1、对于处理验证码，爬虫爬久了通常网站的处理策略就是让你输入验证码验证是否机器人，此时有三种解决方法：

第一种把验证码 down 到本地之后，手动输入验证码验证，此种成本相对较高，而且不能完全做到自动抓取，需要人为干预。第二种图像识别验证码，自动填写验证，但是现在的情况是大部分验证码噪声较多复杂度大，对于像我这样对图像识别不是很熟悉的人很难识别出正确的验证码。第三种也是最实用的一种，接入自动打码平台，个人感觉比上两种方法好些。 2、多账号反爬，有很多的网站会通过同一个用户单位时间内操作频次来判断是否机器人，比如像新浪微博等网站。这种情况下我们就需要先测试单用户抓取阈值，然后在阈值前切换账号其他用户，如此循环即可。当然，新浪微博反爬手段不止是账号，还包括单 ip 操作频次等。

3、分布式爬虫，分布式能在一定程度上起到反爬虫的作用，当然相对于反爬虫分布式最大的作用还是能做到高效大量的抓取。

4、保存 cookies，记录用户的状态，在模拟登陆十分麻烦的情况下，我们不妨直接在 web 上登陆之后取下 cookie 并保存然后带上 cookie 做爬虫，但这不是长久的方法，而且 cookie 隔一段时间可能失效。有的网站会根据 cookie 中的一些值去判断是否机器人，这个需要自己不断测试，比如豆瓣。

5、注意配合移动端、web 端以及桌面版，其中 web 端包括 m 站即手机站和 pc 站，往往是 pc 站的模拟抓取难度大于手机站，所以在 m 站和 pc 站的资源相同的情况下优先考虑抓取 m 站。同时如果无法在 web 端抓取，不可忽略在 app 以及桌面版的也可以抓取到目标数据资源。

应对反爬虫的策略，首先要发现网站的反爬虫手段是什么？这个发现的过程就是不断测试的过程，有点类似于 A/B 测试，弄清楚它的反爬虫机制，就成功了一大半了。
