![default](https://user-images.githubusercontent.com/5803001/45228854-de88b400-b2f6-11e8-9ab0-d393ed19f21f.png)

# 关系型数据库中的事务管理、并发控制与日志管理

# 事务基础

## ACID

事务提供一种全做，或不做(All or Nothing)的机制，即将一个活动涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作只有在所有操作均能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚。数据库事务具有 ACID 属性，即原子性(Atomic)、一致性(Consistency)、隔离性(Isolation)、持久性(Durability)，在[分布式事务 https://url.wx-coder.cn/7p8Xx ](https://url.wx-coder.cn/7p8Xx)中我们也会讨论分布式系统中应该如何实现事务机制。

ACID 包含了描述事务操作的整体性的原子性，描述事务操作下数据的正确性的一致性，描述事务并发操作下数据的正确性的隔离性，描述事务对数据修改的可靠性的持久性。针对数据库的一系列操作提供了一种从失败状态恢复到正常状态的方法，使数据库在异常状态下也能够保持数据的一致性，且面对并发访问时，数据库能够提供一种隔离方法，避免彼此间的操作互相干扰。

- 原子性（Atomicity）：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。例如：银行转账，从 A 账户转 100 元至 B 账户，分为两个步骤：从 A 账户取 100 元；存入 100 元至 B 账户。这两步要么一起完成，要么一起不完成。

- 一致性（Consistency）：在事务开始之前和事务结束以后，数据库数据的一致性约束没有被破坏；即当事务 A 与 B 同时运行，无论 A，B 两个事务的结束顺序如何，数据库都会达到统一的状态。

- 隔离性（Isolation）：数据库允许多个并发事务同时对数据进行读写和修改的能力，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 例如：现有有个交易是从 A 账户转 100 元至 B 账户，在这个交易事务还未完成的情况下，如果此时 B 查询自己的账户，是看不到新增加的 100 元的。

- 持久性（Durability）：当某个事务一旦提交，无论数据库崩溃还是其他未知情况，该事务的结果都能够被持久化保存下来。

## 隔离级别

SQL 标准定义了 4 类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。

| 隔离级别                   | 脏读(Dirty Read ) | 不可重复读(NonRepeatable Read ) | 幻读(Phantom Read ) |
| -------------------------- | ----------------- | ------------------------------- | ------------------- |
| 未提交读(Read Uncommitted) | 可能              | 可能                            | 可能                |
| 提交读(Read Committed )    | 不可能            | 可能                            | 可能                |
| 可重复读(Repeatable Read ) | 不可能            | 不可能                          | 可能                |
| 可串行化(Serializable )    | 不可能            | 不可能                          | 不可能              |

### Read Uncommitted | 未提交读

在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读(Dirty Read)。

### Read Committed 提交读

这是大多数数据库系统的默认隔离级别比如 Sql Server, Oracle 等，但不是[MySQL](http://lib.csdn.net/base/14)默认的。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别也支持所谓的不可重复读(Nonrepeatable Read)，因为同一事务的其他实例在该实例处理其间可能会有新的 Commit，所以同一查询可能返回不同结果。

### Repeatable Read | 重复读

当隔离级别设置为 Repeatable Read 时，可以避免不可重复读。不可重复读是指事务 T1 读取数据后，事务 T2 执行更新操作，使 T1 无法再现前一次读取结果。具体地讲，不可重复读包括三种情况：

- 事务 T1 读取某一数据后，事务 T2 对其做了修改，当事务 T1 再次读该数据时，得到与前一次不同的值。例如，T1 读取 B=100 进行运算，T2 读取同一数据 B，对其进行修改后将 B=200 写回数据库。T1 为了对读取值校对重读 B，B 已为 200，与第一次读取值不一致。
- 事务 T1 按一定条件从数据库中读取了某些数据记录后，事务 T2 删除了其中部分记录，当 T1 再次按相同条件读取数据时，发现某些记录神密地消失了。
- 事务 T1 按一定条件从数据库中读取某些数据记录后，事务 T2 插入了一些记录，当 T1 再次按相同条件读取数据时，发现多了一些记录，也就是幻读。

这是 MySQL 的默认事务隔离级别，它确保在一个事务内的相同查询条件的多次查询会看到同样的数据行，都是事务开始时的数据快照。虽然 Repeatable Read 避免了不可重复读，但还有可能出现幻读。简单说，就是当某个事务在读取某个范围内的记录时，另外的一个事务又在该范围内插入新的记录。在之前的事务在读取该范围的记录时，就会产生幻行，InnoDB 通过间隙锁(next-key locking)策略防止幻读的出现。

### Serializable | 序列化

Serializable 是最高的事务隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。该隔离级别代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻读。

# 并发控制

并发控制旨在针对数据库中对事务并行的场景，保证 ACID 中的一致性（Consistency）与隔离性（Isolation）。假如所有的事务都仅进行数据读取，那么事务之间并不会有冲突；而一旦某个事务读取了正在被其他事务修改的数据或者两个事务修改了相同的数据，那么数据库就必须来保证事务之间的隔离，来避免某个事务因为未见最新的数据而造成的误操作。解决并发控制问题最理想的方式就是能够每当某个事务被创建或者停止的时候，监控所有事务的所有操作，判断是否存在冲突的事务，然后对冲突事务中的操作进行重排序以尽可能少地减少冲突，而后以特定的顺序运行这些操作。绝大部分数据库会采用锁（Locks）或者数据版本控制（Data Versioning）的方式来处理并发控制问题。

数据库技术中主流的三种并发控制技术分别是： Multi-version Concurrency Control (MVCC), Strict Two-Phase Locking (S2PL), 以及 Optimistic Concurrency Control (OCC)，每种技术也都有很多的变种。在 MVCC 中，每次写操作都会在旧的版本之上创建新的版本，并且会保留旧的版本。当某个事务需要读取数据时，数据库系统会从所有的版本中选取出符合该事务隔离级别要求的版本。MVCC 的最大优势在于读并不会阻塞写，写也不会阻塞读；而像 S2PL 这样的系统，写事务会事先获取到排他锁，从而会阻塞读事务。PostgreSQL 以及 Oracle 等 RDBMS 实际使用了所谓的 Snapshot Isolation（SI）这个 MVCC 技术的变种。Oracle 引入了额外的 Rollback Segments，当写入新的数据时，老版本的数据会被写入到 Rollback Segment 中，随后再被覆写到实际的数据块。PostgreSQL 则是使用了相对简单的实现方式，新的数据对象会被直接插入到关联的 Table Page 中；而在读取表数据的时候，PostgreSQL 会通过可见性检测规则（Visibility Check Rules）来选择合适的版本。

## 锁管理器（Lock Manager）

基于锁的方式基础理念为：如果某个事务需要数据，则对数据加锁，操作完毕后释放锁；如果过程中其他事务需要锁，则需要等到该事务释放数据锁，这种锁也就是所谓的排他锁（Exclusive Lock）。不过使用排他锁会带来极大的性能损耗，其会导致其他那些仅需要读取数据的事务也陷入等待。从锁的策略上，可以分为共享锁与排他锁：

- 共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。
- 排他锁又称写锁，如果事务 T 对数据 A 加上排他锁后，则其他事务不能再对 A 加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。

从锁定的数据范围锁粒度（Lock Granularity）来看分为：

- 表锁：管理锁的开销最小，同时允许的并发量也最小的锁机制。MyIsam 存储引擎使用的锁机制。当要写入数据时，把整个表都锁上，此时其他读、写动作一律等待。在 MySql 中，除了 MyIsam 存储引擎使用这种锁策略外，MySql 本身也使用表锁来执行某些特定动作，比如 ALTER TABLE.
- 行锁：可以支持最大并发的锁策略。InnoDB 和 Falcon 两种存储引擎都采用这种策略。

在 [MySQL 实战 https://url.wx-coder.cn/Tu5dq ](https://url.wx-coder.cn/Tu5dq)中我们也讨论了如何触发锁机制，譬如查询加锁，`select * from testlock where id=1 for update;`，即查询时不允许更改，该语句在自动提交为 off 或事务中生效，相当于更改操作，模拟加锁；而更像类操作 `update testlock name=name;` 则是会自动加锁。

## MVCC

在[并发编程导论 https://url.wx-coder.cn/Yagu8 ](https://url.wx-coder.cn/Yagu8)中我们讨论了两种不同类型的锁：乐观锁（Optimistic Lock）与悲观锁（Pessimistic Lock），前文介绍的各种锁即是悲观锁，而 MVCC(Multiple Version Concurrency Control) 这样的基于数据版本的锁则是乐观锁，它能够保证读写操作之间不会相互阻塞：

- 每个事务都可以在同一时间修改相同的数据；
- 每个事务会保有其需要的数据副本；
- 如果两个事务修改了相同的数据，那么仅有单个更改操作会被接收，另一个操作会被回滚或者重新执行。

乐观锁，大多是基于数据版本（Version）记录机制实现。数据版本即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 `version` 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

[![image.png](https://i.postimg.cc/nzP3gcmw/image.png)](https://postimg.cc/hzV87np9)

PostgreSQL 中则是依赖于 txid 以及 Commit Log 结合而成的可见性检测机制来实现 MVCC，详情可以参考 [PostgreSQL 架构机制 https://url.wx-coder.cn/SgRDQ ](https://url.wx-coder.cn/SgRDQ)中关于并发控制相关的介绍。

# 日志管理器（Log Manager）

数据库事务由具体的 DBMS 系统来保障操作的原子性，同一个事务当中，如果有某个操作执行失败，则事务当中的所有操作都需要进行回滚，回到事务执行前的状态。导致事务失败的原因有很多，可能是因为修改不符合表的约束规则，也有可能是网络异常，甚至是存储介质故障等，而一旦事务失败，则需要对所有已作出的修改操作进行还原，使数据库的状态恢复到事务执行前的状态，以保障数据的一致性，使修改操作要么全部成功、要么全部失败，避免存在中间状态。

为了实现数据库状态的恢复，DBMS 系统通常需要维护事务日志以追踪事务中所有影响数据库数据的操作，以便执行失败时进行事务的回滚。以 MySQL 的 innodb 存储引擎为例，innodb 存储引擎通过预写事务日志的方式，来保障事务的原子性、一致性以及持久性。它包含 redo 日志和 undo 日志，redo 日志在系统需要的时候，对事务操作进行重做，如当系统宕机重启后，能够对内存中还没有持久化到磁盘的数据进行恢复，而 undo 日志，则能够在事务执行失败的时候，利用这些 undo 信息，将数据还原到事务执行前的状态。

事务日志可以提高事务执行的效率，存储引擎只需要将修改行为持久到事务日志当中，便可以只对该数据在内存中的拷贝进行修改，而不需要每次修改都将数据回写到磁盘。这样做的好处是，日志写入是一小块区域的顺序 I/O，而数据库数据的磁盘回写则是随机 I/O，磁头需要不停地移动来寻找需要更新数据的位置，无疑效率更低，通过事务日志的持久化，既保障了数据存储的可靠性，又提高了数据写入的效率。

# WAL(Write Ahead Logging)
