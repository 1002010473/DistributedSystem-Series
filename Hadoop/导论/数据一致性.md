



在分布式系统中，为了保证数据的可靠性与性能，我们不可避免的对数据进行复制与多节点存储，而数据一致性主要为了解决分布式多个存储节点情况下怎么保证逻辑上相同的副本能够返回相同的数据。

> [保证分布式系统数据一致性的6种方案 ](http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&mid=2653546976&idx=1&sn=c3fb2338389a41e7ab998c0c21bd3e5d&scene=23&srcid=0419TGxOI4Nac6fHhWRNDeR0#rd)

分布式系统常常通过数据的复制来提高系统的可靠性和容错性，并且将数据的副本存放到不同的机器上，由于多个副本的存在，使得维护副本一致性的代价很高。因此，许多分布式系统都采用弱一致性或者是最终一致性，来提高系统的性能和吞吐能力，这样不同的一致性模型也相继被提出。

强一致性要求无论数据的更新操作是在哪个副本上执行，之后所有的读操作都要能够获取到更新的最新数据。对于单副本的数据来说，读和写都是在同一份数据上执行，容易保证强一致性，但对于多副本数据来说，若想保障强一致性，就需要等待各个副本的写入操作都执行完毕，才能提供数据的读取，否则就有可能数据不一致，这种情况需要通过分布式事务来保证操作的原子性，并且外界无法读到系统的中间状态。
弱一致性指的是系统的某个数据被更新后，后续对该数据的读取操作，取到的可能是更新前的值，也可能是更新后的值，全部用户完全读取到更新后的数据，需要经过一段时间，这段时间称作“不一致性窗口”。
最终一致性是弱一致性的一种特殊形式，这种情况下系统保证用户最终能够读取到某个操作对系统的更新，“不一致性窗口”的时间依赖于网络的延迟、系统的负载以及副本的个数。

分布式系统中采用最终一致性的例子很多，如MySQL数据库的主/从数据同步，ZooKeeper的Leader Election和Atomic Broadcas等。

# 数据一致性模型
- [分布式系统一致性的发展历史](http://36kr.com/p/5037166.html)

[严格一致性](http://www.sigma.me/2011/05/6/strict-consistency.html)其实从物理定律上来说就是不能实现的(它要求写操作能够瞬间传播出去)。

[顺序一致性](http://www.sigma.me/2011/05/6/sequential-consistency.html)是可行的，在编程人员中很流行且广泛应用。但是它的性能很差。解决这个问题的方法是放宽一致性的模型。下表以限制程度递减的顺序给出了几种可能的模型。

| 一致性  | 说明                                       |
| ---- | ---------------------------------------- |
| 严格   | 所有的共享访问事件都有绝对时间顺序                        |
| 顺序   | 所有进程都以相同的顺序检测到所有的共享访问事件                  |
| 因果   | 所有进程都以相同的顺序检测到所有因果联系的事件                  |
| 处理器  | PRAM一致性+存储相关性                            |
| PRAM | 所有的进程按照预定的顺序检测到来自一个处理器的写操作，来自其他处理器的写操作不必以相同的顺序出现 |

另一措施是引入了明确的同步变量，正如[弱一致性](http://www.sigma.me/2011/05/6/weak-consistency.html)，[释放一致性](http://www.sigma.me/2011/05/6/release-consistency.html)和[入口一致性](http://www.sigma.me/2011/05/6/entry-consistency.html)所做的那样。下表总结了这三种模式。当进程对普通共享数据变量执行操作时，不能保证它们何时对于其他进程是可见的。只有当访问同步变量后，变化才能传播出去。

| 一致性  | 说明                       |
| ---- | ------------------------ |
| 弱    | 同步完成后，共享数据才可能保持一致        |
| 释放   | 当离开临界区时，共享数据就保持一致        |
| 入口   | 当进入临界区时，和该临界区相关的共享数据保持一致 |

这三种模型的不同在于其同步机制如何工作。但在这三种情况中，它们都可在一个临界区中执行多重的读写操作，而不引起数据传输。当临界区的操作完成后，最后结果或者广播发送给其他进程或准备就绪在其他进程需要时再发送出去。




## 数据为中心的一致性模型
为了讨论的方便，下面先规定一下两个有关读写的表示法：
1. Write(y,a)表示向变量y写入数据a；
2. Read(x,b)表示从变量x读出数据b；
### 强一致性/原子一致性/线性一致性

所谓的强一致性(strict consistency),也称为原子一致性(atomic consistency)或者线性(Linearizability)。
它对一致性的要求两个：
1. 任何一次读都能读到某个数据的最近一次写的数据。
2. 系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。 
显然这两个条件都对全局时钟有非常高的要求。强一致性，只是存在理论中的一致性模型，比它要求更弱一些的，就是顺序一致性。
### 顺序一致性
顺序一致性(Sequential Consistency)，也同样有两个条件，其一与前面强一致性的要求一样，也是可以马上读到最近写入的数据，然而它的第二个条件就弱化了很多，它允许系统中的所有进程形成自己合理的统一的一致性，不需要与全局时钟下的顺序都一致。

这里的第二个条件的要点在于：

1. 系统的所有进程的顺序一致，而且是合理的，就是说任何一个进程中，这个进程对同一个变量的读写顺序要保持，然后大家形成一致。
2. 不需要与全局时钟下的顺序一致。

可见，顺序一致性在顺序要求上并没有那么严格，它只要求系统中的所有进程达成自己认为的一致就可以了，即错的话一起错，对的话一起对，同时不违反程序的顺序即可，并不需要个全局顺序保持一致。

以下面的图来看看这两种一致性的对比：

![](http://mmbiz.qpic.cn/mmbiz/vxCq1iahXotiaFs84SvDRF5U3gefsfA2F8XDuktqj8WCQ2ohQW1lFia0b8UrMXgYySHibAPGatSIKa4avl9N4Z4lfA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

(出自《分布式计算-原理、算法与系统》)

1. 图a是满足顺序一致性，但是不满足强一致性的。原因在于，从全局时钟的观点来看，P2进程对变量X的读操作在P1进程对变量X的写操作之后，然而读出来的却是旧的数据。但是这个图却是满足顺序一致性的，因为两个进程P1,P2的一致性并没有冲突。从这两个进程的角度来看，顺序应该是这样的：Write(y,2) , Read(x,0) , Write(x,4), Read(y,2)，每个进程内部的读写顺序都是合理的，但是显然这个顺序与全局时钟下看到的顺序并不一样。
2. 图b满足强一致性，因为每个读操作都读到了该变量的最新写的结果，同时两个进程看到的操作顺序与全局时钟的顺序一样，都是Write(y,2) , Read(x,4) , Write(x,4), Read(y,2)。
3. 图c不满足顺序一致性，当然也就不满足强一致性了。因为从进程P1的角度看，它对变量Y的读操作返回了结果0。那么就是说，P1进程的对变量Y的读操作在P2进程对变量Y的写操作之前，这意味着它认为的顺序是这样的：write(x,4) , Read(y,0) , Write(y,2), Read(x,0)，显然这个顺序又是不能被满足的，因为最后一个对变量x的读操作读出来也是旧的数据。因此这个顺序是有冲突的，不满足顺序一致性。

### 因果一致性
因果一致性(Casual Consistency)在一致性的要求上，又比顺序一致性降低了：它仅要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。

因果相关的要求是这样的：

1. 本地顺序：本进程中，事件执行的顺序即为本地因果顺序。
2. 异地顺序：如果读操作返回的是写操作的值，那么该写操作在顺序上一定在读操作之前。
3. 闭包传递：和时钟向量里面定义的一样，如果a->b，b->c，那么肯定也有a->c。

以下面的图来看看这两种一致性的对比：

![](http://mmbiz.qpic.cn/mmbiz/vxCq1iahXotiaFs84SvDRF5U3gefsfA2F820z5wc1aTap2rbuoaicibD2NOLbxdxiczANICF3upelico3emmn93IwnGQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

(出自《分布式计算-原理、算法与系统》)

1. 图a满足顺序一致性，因此也满足因果一致性，因为从这个系统中的四个进程的角度看，它们都有相同的顺序也有相同的因果关系。
2. 图b满足因果一致性但是不满足顺序一致性，这是因为从进程P3、P4看来，进程P1、P2上的操作因果有序，因为P1、P2上的写操作不存在因果关系，所以它们可以任意执行。不满足一致性的原因，同上面一样是可以推导出冲突的情况来。


在infoq分享的腾讯朋友圈的设计中，他们在设计数据一致性的时候，使用了因果一致性这个模型。用于保证对同一条朋友圈的回复的一致性，比如这样的情况：

1. A发了朋友圈内容为梅里雪山的图片。
2. B针对内容a回复了评论：“这里是哪里？”
3. C针对B的评论进行了回复：”这里是梅里雪山“。

那么，这条朋友圈的显示中，显然C针对B的评论，应该在B的评论之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。

微信的做法是：

1. 每个数据中心，都自己生成唯一的、递增的数据ID，确保能排重。在下图的示例中，有三个数据中心，数据中心1生成的数据ID模1为0，数据中心1生成的数据ID模2为0，数据中心1生成的数据ID模3为0，这样保证了三个数据中心的数据ID不会重复全局唯一。
2. 每条评论都比本地看到所有全局ID大，这样来确保因果关系，这部分的原理前面提到的向量时钟一样。

![](http://mmbiz.qpic.cn/mmbiz/vxCq1iahXotiaFs84SvDRF5U3gefsfA2F8cp2O082gPUZEbkiawXfogQ3DI8ghhhtFZqicbatRvrklGwxe8JlmrlOw/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

有了这个模型和原理，就很好处理前面针对评论的评论的顺序问题了。

1. 假设B在数据中心1上，上面的ID都满足模1为0，那么当B看到A的朋友圈时，发表了评论，此时给这个评论分配的ID是1，因此B的时钟向量数据是[1]。
2. 假设C在数据中心2上，上面的ID都满足模2为0，当C看到了B的评论时，针对这个评论做了评论，此时需要给这个评论分配的ID肯定要满足模2为0以及大于1，评论完毕之后C上面的时钟向量是[1,2]。
3. 假设A在数据中心3上，上面的ID都满足模3为0，当A看到B、C给自己的评论时，很容易按照ID进行排序和合并--即使A在收到C的数据[1,2]之后再收到B的数据[1]，也能顺利的完成合并。

1.严格一致性(strict consistency)

对于数据项x的任何读操作将返回最近一次对x进行写操作的结果所对应的值。

严格一致性是限制性最强的模型，但是在分布式系统中实现这种模型代价太大，所以在实际系统中运用有限。

 

2.顺序一致性

任何执行结果都是相同的，就好像所有进程对数据存储的读、写操作是按某种序列顺序执行的，并且每个进程的操作按照程序所制定的顺序出现在这个序列中。

也就是说，任何读、写操作的交叉都是可接受的，但是所有进程都看到相同的操作交叉。顺序一致性由Lamport(1979)在解决多处理器系统的共享存储器时首次提出的。

 

3.因果一致性

所有进程必须以相同的顺序看到具有潜在因果关系的写操作。不同机器上的进程可以以不同的顺序看到并发的写操作(Hutto和Ahamad 1990)。

假设P1和P2是有因果关系的两个进程，例如P2的写操作信赖于P1的写操作，那么P1和P2对x的修改顺序，在P3和P4看来一定是一样的。但如果P1和P2没有关系，那么P1和P2对x的修改顺序，在P3和P4看来可以是不一样的。

 

下表列出一些一致性模型，以限制性逐渐降低的顺序排列。

| 一致性  | 描述                                       |
| ---- | ---------------------------------------- |
| 严格   | 所有共享访问按绝对时间排序                            |
| 线性化  | 所有进程以相同的顺序看到所有的共享访问。而且，访问是根据(非唯一的)全局时间戮排序的 |
| 顺序   | 所有进程以相同顺序看到所有的共享访问。访问不是按时间排序的            |
| 因果   | 所有的进程以相同的顺序看到困果相关的共享访问                   |
| FIFO | 所有进程以不同的进程提出写操作的顺序相互看到写操作。来自不同的进程的写操作可以不必总是以同样的顺序出现 |

 

另一种不同的方式是引入显式的同步变量，下表是这样的一致性模型的总结。

| 一致性  | 描述                       |
| ---- | ------------------------ |
| 弱    | 只有在执行一次同步后，共享数据才被认为是一致的  |
| 释放   | 退出临界区时，使共享数据成为一致         |
| 入口   | 进入临界区时，使属于一个临界区的共享数据成为一致 |

 

以客户为中心的一致性模型

1.最终一致性

最终一致性指的是在一段时间内没有数据更新操作的话，那么所有的副本将逐渐成为一致的。例如OpenStack Swift就是采用这种模型。以一次写多次读的情况下，这种模型可以工作得比较好。

 

2.单调读

如果一个进程读取数据项x的值，那么该进程对x执行的任何后续读操作将总是得到第一次读取的那个值或更新的值

 

3.单调写

一个进程对数据x执行的写操作必须在该进程对x执行任何后续写操作之前完成。

 

4.写后读

一个进程对数据x执行一次写操作的结果总是会被该进程对x执行的后续读操作看见。

 

5.读后写

同一个进程对数据项x执行的读操作之后的写操作，保证发生在与x读取值相同或比之更新的值上。






# Distributed Transcation:分布式事务

``` 
分布式事务往往和本地事务进行对比分析，以支付宝转账到余额宝为例，假设有：

支付宝账户表：A(id，userId，amount)　　
```

　　余额宝账户表：B(id，userId，amount)

　　用户的userId=1；

　　从支付宝转账1万块钱到余额宝的动作分为两步：

　　1)支付宝表扣除1万：update A set amount=amount-10000 where userId=1;

　　2)余额宝表增加1万：update B set amount=amount+10000 where userId=1;

``` sql
Begin transaction
         update A set amount=amount-10000 where userId=1;
         update B set amount=amount+10000 where userId=1;
End transaction
commit;
```

``` 
在Spring中使用一个@Transactional事务也可以搞定上述的事务功能：
```

``` java
@Transactional(rollbackFor=Exception.class)
    public void update() {
        updateATable(); //更新A表
        updateBTable(); //更新B表
    }
```

``` 
如果系统规模较小，数据表都在一个数据库实例上，上述本地事务方式可以很好地运行，但是如果系统规模较大，比如支付宝账户表和余额宝账户表显然不会在同一个数据库实例上，他们往往分布在不同的物理节点上，这时本地事务已经失去用武之地。
```

## 两阶段提交协议

``` 
两阶段提交协议(Two-phase Commit，2PC)经常被用来实现分布式事务。一般分为协调器C和若干事务执行者Si两种角色，这里的事务执行者就是具体的数据库，协调器可以和事务执行器在一台机器上。
```

![](http://images0.cnblogs.com/blog2015/522490/201508/091642197846523.png)

1) 我们的应用程序(client)发起一个开始请求到TC；

2) TC先将<prepare>消息写到本地日志，之后向所有的Si发起<prepare>消息。以支付宝转账到余额宝为例，TC给A的prepare消息是通知支付宝数据库相应账目扣款1万，TC给B的prepare消息是通知余额宝数据库相应账目增加1w。为什么在执行任务前需要先写本地日志，主要是为了故障后恢复用，本地日志起到现实生活中凭证 的效果，如果没有本地日志(凭证)，容易死无对证；

　　3) Si收到<prepare>消息后，执行具体本机事务，但不会进行commit，如果成功返回<yes>，不成功返回<no>。同理，返回前都应把要返回的消息写到日志里，当作凭证。

　　4) TC收集所有执行器返回的消息，如果所有执行器都返回yes，那么给所有执行器发生送commit消息，执行器收到commit后执行本地事务的commit操作；如果有任一个执行器返回no，那么给所有执行器发送abort消息，执行器收到abort消息后执行事务abort操作。

　　注：TC或Si把发送或接收到的消息先写到日志里，主要是为了故障后恢复用。如某一Si从故障中恢复后，先检查本机的日志，如果已收到<commit >，则提交，如果<abort >则回滚。如果是<yes>，则再向TC询问一下，确定下一步。如果什么都没有，则很可能在<prepare>阶段Si就崩溃了，因此需要回滚。

　　现如今实现基于两阶段提交的分布式事务也没那么困难了，如果使用java，那么可以使用开源软件atomikos([http://www.atomikos.com/](http://www.atomikos.com/))来快速实现。

　　不过但凡使用过的上述两阶段提交的同学都可以发现性能实在是太差，根本不适合高并发的系统。为什么？

　　1)两阶段提交涉及多次节点间的网络通信，通信时间太长！

　　2)事务时间相对于变长了，锁定的资源的时间也变长了，造成资源等待时间也增加好多！



## 消息队列方式

``` 
比如在北京很有名的姚记炒肝点了炒肝并付了钱后，他们并不会直接把你点的炒肝给你，往往是给你一张小票，然后让你拿着小票到出货区排队去取。为什么他们要将付钱和取货两个动作分开呢？原因很多，其中一个很重要的原因是为了使他们接待能力增强(并发量更高)。
```

　　还是回到我们的问题，只要这张小票在，你最终是能拿到炒肝的。同理转账服务也是如此，当支付宝账户扣除1万后，我们只要生成一个凭证(消息)即可，这个凭证(消息)上写着“让余额宝账户增加 1万”，只要这个凭证(消息)能可靠保存，我们最终是可以拿着这个凭证(消息)让余额宝账户增加1万的，即我们能依靠这个凭证(消息)完成最终一致性。

### 业务与消息耦合的方式

``` 
支付宝在完成扣款的同时，同时记录消息数据，这个消息数据与业务数据保存在同一数据库实例里(消息记录表表名为message)；
```

``` sql
Begin transaction
         update A set amount=amount-10000 where userId=1;
         insert into message(userId, amount,status) values(1, 10000, 1);
End transaction
commit;
```

``` 
上述事务能保证只要支付宝账户里被扣了钱，消息一定能保存下来。当上述事务提交成功后，我们通过实时消息服务将此消息通知余额宝，余额宝处理成功后发送回复成功消息，支付宝收到回复后删除该条消息数据。
```

### 业务与消息解耦方式

``` 
上述保存消息的方式使得消息数据和业务数据紧耦合在一起，从架构上看不够优雅，而且容易诱发其他问题。为了解耦，可以采用以下方式。
```

　　1)支付宝在扣款事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不真正发送，只有消息发送成功后才会提交事务；

　　2)当支付宝扣款事务被提交成功后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才真正发送该消息；

　　3)当支付宝扣款事务提交失败回滚后，向实时消息服务取消发送。在得到取消发送指令后，该消息将不会被发送；

　　4)对于那些未确认的消息或者取消的消息，需要有一个消息状态确认系统定时去支付宝系统查询这个消息的状态并进行更新。为什么需要这一步骤，举个例子：假设在第2步支付宝扣款事务被成功提交后，系统挂了，此时消息状态并未被更新为“确认发送”，从而导致消息不能被发送。

　　优点：消息数据独立存储，降低业务系统与消息系统间的耦合；

　　缺点：一次消息发送需要两次请求；业务处理服务需要实现消息状态回查接口。

### 消息重复投递

``` 
还有一个很严重的问题就是消息重复投递，以我们支付宝转账到余额宝为例，如果相同的消息被重复投递两次，那么我们余额宝账户将会增加2万而不是1万了。	 	为什么相同的消息会被重复投递？比如余额宝处理完消息msg后，发送了处理成功的消息给支付宝，正常情况下支付宝应该要删除消息msg，但如果支付宝这时候悲剧的挂了，重启后一看消息msg还在，就会继续发送消息msg。

解决方法很简单，在余额宝这边增加消息应用状态表(message_apply)，通俗来说就是个账本，用于记录消息的消费情况，每次来一个消息，在真正执行之前，先去消息应用状态表中查询一遍，如果找到说明是重复消息，丢弃即可，如果没找到才执行，同时插入到消息应用状态表(同一事务)。
```

\``` sql list

for each msg in queue

  Begin transaction

``` 
select count(*) as cnt from message_apply where msg_id=msg.msg_id;
if cnt==0 then
  update B set amount=amount+10000 where userId=1;
  insert into message_apply(msg_id) values(msg.msg_id);
```

  End transaction

  commit;

\```

# 流处理方案
当系统变得越来越复杂，数据库会被拆分为多个更小的库，如果借助这些衍生库实现像全文搜索这样的功能，那么如何保证所有的数据保持同步就是一项很有挑战性的任务了。使用多个数据库时，最大的问题在于它们并不是互相独立的。相同的数据会以不同的形式进行存储，所以当数据更新的时候，具有对应数据的所有数据库都需要进行更新。保证数据同步的最常用方案就是将其视为应用程序逻辑的责任，通常会对每个数据库进行独立的写操作。这是一个脆弱的方案，如果发生像网络故障或服务器宕机这样的失败场景，那么对一些数据库的更新可能会失败，从而导致这些数据库之间出现不一致性。Kleppmann 认为这并不是能够进行自我纠正的最终一致性，至少相同的数据再次进行写操作之前，无法实现一致性。

在leader(主)数据库中，同时会将所有的写入操作按照处理的顺序存储为流，然后一个或多个follower 数据库就能读取这个流并按照完全相同的顺序执行写入。这样的话，这些数据库就能更新自己的数据并成为leader 数据库的一致性备份。对于Kleppmann 来说，这是一个非常具有容错性的方案。每个follower 都遵循它在流中的顺序，在出现网络故障或宕机时，follower 数据库能够从上一次的保存点开始继续进行处理。Kleppmann 还提到在实现上述场景时，使用Kafka 作为工具之一。目前，他正在编写一个实现，Bottled Water，在这个实现中，他使用了PostgreSQL 来抽取数据变化，然后将其中继到Kafka 中，代码可以在[GitHub](https://github.com/confluentinc/bottledwater-pg) 上获取到。

# Distributed ID
## Snowflake
- [Unique Id generator](https://github.com/mumrah/flake-java)
- [Twitter Snowflake Official Repo](https://github.com/twitter/snowflake)

Twitter-Snowflake算法产生的背景相当简单，为了满足Twitter每秒上万条消息的请求，每条消息都必须分配一条唯一的id，这些id还需要一些大致的顺序(方便客户端排序)，并且在分布式系统中不同机器产生的id必须不同。把时间戳，工作机器id，序列号组合在一起。
![](http://121.40.136.3/wp-content/uploads/2015/04/snowflake-64bit.jpg)
除了最高位bit标记为不可用以外，其余三组bit占位均可浮动，看具体的业务需求而定。默认情况下41bit的时间戳可以支持该算法使用到2082年，10bit的工作机器id可以支持1023台机器，序列号支持1毫秒产生4095个自增序列id。

### 时间戳
这里时间戳的细度是毫秒级，具体代码如下，建议使用64位linux系统机器，因为有vdso，gettimeofday()在用户态就可以完成操作，减少了进入内核态的损耗。
```
uint64_t generateStamp()
{
    timeval tv;
    gettimeofday(&tv, 0);
    return (uint64_t)tv.tv_sec * 1000 + (uint64_t)tv.tv_usec / 1000;
}

```
默认情况下有41个bit可以供使用，那么一共有T(1llu << 41)毫秒供你使用分配，年份 = T / (3600 * 24 * 365 * 1000) = 69.7年。如果你只给时间戳分配39个bit使用，那么根据同样的算法最后年份 = 17.4年。总体来说，是一个很高效很方便的GUID产生算法，一个int64_t字段就可以胜任，不像现在主流128bit的GUID算法，即使无法保证严格的id 序列性，但是对于特定的业务，比如用做游戏服务器端的GUID产生会很方便。另外，在多线程的环境下，序列号使用atomic可以在代码实现上有效减少锁 的密度。

### 工作机器ID

严格意义上来说这个bit段的使用可以是进程级，机器级的话你可以使用MAC地址来唯一标示工作机器，工作进程级可以使用IP+Path来区分工作进程。如果工作机器比较少，可以使用配置文件来设置这个id是一个不错的选择，如果机器过多配置文件的维护是一个灾难性的事情。
这里的解决方案是需要一个工作id分配的进程，可以使用自己编写一个简单进程来记录分配id，或者利用Mysql auto_increment机制也可以达到效果。
![](http://121.40.136.3/wp-content/uploads/2015/04/snowflake-%E5%B7%A5%E4%BD%9Cid.jpg)
工作进程与工作id分配器只是在工作进程启动的时候交互一次，然后工作进程可以自行将分配的id数据落文件，下一次启动直接读取文件里的id使用。PS：这个工作机器id的bit段也可以进一步拆分，比如用前5个bit标记进程id，后5个bit标记线程id之类。

### 序列号
序列号就是一系列的自增id(多线程建议使用atomic)，为了处理在同一毫秒内需要给多条消息分配id，若同一毫秒把序列号用完了，则“等待至下一毫秒”。
```

uint64_t waitNextMs(uint64_t lastStamp)
{
    uint64_t cur = 0;
    do {
        cur = generateStamp();
    } while (cur <= lastStamp);
    return cur;
}

```



